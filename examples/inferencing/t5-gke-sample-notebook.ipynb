{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4c9f7-632c-4a37-aa98-7aac0a805e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tritonclient[http] transformers torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3d36b44-a320-440c-9764-8bee1c1a1396",
   "metadata": {},
   "source": [
    "Make sure that your notebook is on the same VPC as your GKE cluster.\n",
    "\n",
    "We can use both the /infer endpoint and the direct triton endpoint with the Triton Client to get inferencing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f4bf1-ce92-46a4-ac9a-1d85c18780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate these from the logs of the GKE-Provision-Deploy image, or substitute other known values\n",
    "host = \"\"\n",
    "flask_node_port = \"\"\n",
    "triton_node_port = \"\"\n",
    "payload = \"Sandwiched between a second-hand bookstore and record shop in Cape Town's charmingly grungy suburb of Observatory is a blackboard reading 'Tapi Tapi -- Handcrafted, authentic African ice cream.' The parlor has become one of Cape Town's most talked about food establishments since opening in October 2020. And in its tiny kitchen, Jeff is creating ice cream flavors like no one else. Handwritten in black marker on the shiny kitchen counter are today's options: Salty kapenta dried fish (blitzed), toffee and scotch bonnet chile Sun-dried blackjack greens and caramel, Malted millet ,Hibiscus, cloves and anise. Using only flavors indigenous to the African continent, Guzha's ice cream has become the tool through which he is reframing the narrative around African food. 'This (is) ice cream for my identity, for other people's sake,' Jeff tells CNN. 'I think the (global) food story doesn't have much space for Africa ... unless we're looking at the generic idea of African food,' he adds. 'I'm not trying to appeal to the global universe -- I'm trying to help Black identities enjoy their culture on a more regular basis.'\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81c87d95-8cc9-4ef4-8cd3-a21fb8057bda",
   "metadata": {},
   "source": [
    "# Flask Inferencing\n",
    "\n",
    "The flask endpoint takes inputs in the same format that Vertex Predictions. A list of inputs in the 'instances' property, and a response with a list of predictions in the 'predictions' property.\n",
    "\n",
    "Using the flask endpoint will do tokenization on the server, which makes it great for experimentation but is not capable of maximum throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b47057-fbb0-41cc-8420-19db348af5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "flask_payload = { \"instances\": [payload] }\n",
    "flask_endpoint = f'http://{host}:{flask_node_port}/infer'\n",
    "\n",
    "response = requests.post(flask_endpoint, json=flask_payload)\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ef5e9fd-9032-493e-9c18-d9565a4ef985",
   "metadata": {},
   "source": [
    "# Triton Inferencing\n",
    "\n",
    "The underlying Triton server can also be called directly. Using Triton directly requires management of the payload on the client-side:\n",
    "* Encoding of the payload\n",
    "* Conversion to TensorRT format\n",
    "* Decoding the response payload\n",
    "\n",
    "Removal of the pre/post processing and a reduction of network calls leads to a higher throughput endpoint."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e774bbb-247c-46ef-a159-727802998f47",
   "metadata": {},
   "source": [
    "Pre-load the tokenizer to download the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040cb36e-c817-4cdd-b624-966e177847e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7621ce7-ac14-4e09-b1d4-4535853c31bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate the tensor inputs by:\n",
    "\n",
    "    - Tokenizing your payload\n",
    "\n",
    "    - Setting your desired inference parameters, here you can see an example of setting sequence_length, top_k, and max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88721130-c1bb-44fa-92a6-ca0cd792d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_token = tokenizer(\n",
    "        payload, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "input_ids = input_token.input_ids.numpy().astype(np.uint32)\n",
    "\n",
    "mem_seq_len = (\n",
    "        torch.sum(input_token.attention_mask, dim=1).numpy().astype(np.uint32)\n",
    "    )\n",
    "mem_seq_len = mem_seq_len.reshape([mem_seq_len.shape[0], 1])\n",
    "max_output_len = np.array([[128]], dtype=np.uint32)\n",
    "runtime_top_k = (1.0 * np.ones([input_ids.shape[0], 1])).astype(np.uint32)\n",
    "\n",
    "inputs = [\n",
    "    httpclient.InferInput(\n",
    "        \"input_ids\", input_ids.shape, np_to_triton_dtype(input_ids.dtype)\n",
    "    ),\n",
    "    httpclient.InferInput(\n",
    "        \"sequence_length\",\n",
    "        mem_seq_len.shape,\n",
    "        np_to_triton_dtype(mem_seq_len.dtype),\n",
    "    ),\n",
    "    httpclient.InferInput(\n",
    "        \"max_output_len\",\n",
    "        max_output_len.shape,\n",
    "        np_to_triton_dtype(mem_seq_len.dtype),\n",
    "    ),\n",
    "    httpclient.InferInput(\n",
    "        \"runtime_top_k\",\n",
    "        runtime_top_k.shape,\n",
    "        np_to_triton_dtype(runtime_top_k.dtype),\n",
    "    ),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(input_ids, False)\n",
    "inputs[1].set_data_from_numpy(mem_seq_len, False)\n",
    "inputs[2].set_data_from_numpy(max_output_len, False)\n",
    "inputs[3].set_data_from_numpy(runtime_top_k, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a3522a7-1c0b-475e-bf1a-c26354e31ce9",
   "metadata": {},
   "source": [
    "Create a client to the Triton server running on your cluster. Set `verbose=True` to see the encoded payload being sent to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fd825-1969-4fd7-b533-1b32f77d79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(f\"{host}:{triton_node_port}\", verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "892566a7-981d-4bf2-9c9d-117db2861855",
   "metadata": {},
   "source": [
    "Call client.infer() with the model name and the inputs. Note that the model name is hardcoded as 'fastertransformer' when deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb74fe-521a-44ee-91ca-20acf5bdb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = client.infer(\"fastertransformer\", inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d42ee1b-2142-47e7-866d-4d68dbb12882",
   "metadata": {},
   "source": [
    "The response is returned as tensors as well, so it will need to be decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9539bf7-ba65-48ea-8f21-0ac5099b7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ft_decoding_outputs = result.as_numpy(\"output_ids\")\n",
    "ft_decoding_seq_lens = result.as_numpy(\"sequence_length\")\n",
    "tokens = tokenizer.decode(\n",
    "    ft_decoding_outputs[0][0][: ft_decoding_seq_lens[0][0]],\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
